{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 8053052,
          "sourceType": "datasetVersion",
          "datasetId": 4749322
        }
      ],
      "dockerImageVersionId": 30674,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "DL Assign 2 - PART B",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mparag019/DL-Assignment-2/blob/main/PART-B/DL_Assign_2_PART_B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'dataset2:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4749322%2F8053052%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240407%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240407T142307Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dec75f514e87132b001ad4d147702ca2b48cc20c2703825e397a9e08fd7bd388f13b42a86ec657715b603154b9954b28075a5d8636f56c31d87eb1e5834d2d6cf8c5b5cf7fe41ca6a56be59b2cadf934cad1a2e09db6b4e6dee9ce7e3ae13281e93e07fceb72dd9a5b97ac020229f09935bfcdef7bea78a707c61fb7ee1ed002e3fb5775854d04628d0ab6c4e96fb2f431113df0e3477471973f97b43c9dbe8c6fb00f7452317f6077bfdc572ef15ca3d4f10353fc36d47dce073c7a52cc4051638417e4dae1027646ce5f5a19d7836d21e891bd94d651b7f8b9dbb452ea115bc0ba5b54947bb54417b14b60257369072e8aedf98596f016a7a4a63de1ff3a530'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Nk217bQo8Qpq"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "id": "Nk217bQo8Qpq"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler, random_split, ConcatDataset\n",
        "from torchvision.datasets import ImageFolder\n",
        "import wandb\n",
        "from wandb.sdk.wandb_run import Run\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# Check if CUDA (GPU) is available, and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Set up data transformations\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "# Load the dataset\n",
        "train_data = ImageFolder('/kaggle/input/dataset1/inaturalist_12K/train', transform=train_transforms)\n",
        "test_data = ImageFolder('/kaggle/input/dataset1/inaturalist_12K/val', transform=test_transforms)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "9t_mUtin8Qpr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "9t_mUtin8Qpr"
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of samples in each class\n",
        "class_counts = {}\n",
        "pbar = tqdm(total=len(train_data))\n",
        "for _, label in train_data:\n",
        "    if label not in class_counts:\n",
        "        class_counts[label] = 0\n",
        "    class_counts[label] += 1\n",
        "    pbar.set_postfix()\n",
        "    pbar.update(1)\n",
        "\n",
        "pbar.close()\n",
        "\n",
        "# Calculate the number of samples per class for validation set\n",
        "val_size_per_class = {label: int(count * 0.2) for label, count in class_counts.items()}\n",
        "\n",
        "# Initialize lists to hold indices for train and validation sets\n",
        "train_indices = []\n",
        "val_indices = []\n",
        "\n",
        "# Iterate through the dataset and assign samples to train or validation set\n",
        "pbar = tqdm(total=len(train_data))\n",
        "for idx, (_, label) in enumerate(train_data):\n",
        "    if val_size_per_class[label] > 0:\n",
        "        val_indices.append(idx)\n",
        "        val_size_per_class[label] -= 1\n",
        "    else:\n",
        "        train_indices.append(idx)\n",
        "    pbar.set_postfix()\n",
        "    pbar.update(1)\n",
        "\n",
        "pbar.close()\n",
        "\n",
        "# Create SubsetRandomSampler for train and validation sets\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "val_sampler = SubsetRandomSampler(val_indices)\n",
        ""
      ],
      "metadata": {
        "id": "ZO-Z0piJ8Qpr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ZO-Z0piJ8Qpr"
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate accuracy\n",
        "def calculate_accuracy(outputs, labels):\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == labels).sum().item()\n",
        "    accuracy = correct / labels.size(0)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "DzMVrDRJ8Qps"
      },
      "execution_count": null,
      "outputs": [],
      "id": "DzMVrDRJ8Qps"
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def training_model(epochs, optimizer, criterion, model, train_loader, val_loader):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        training_loss = 0.0\n",
        "        train_accuracy = 0.0\n",
        "        pbar = tqdm(total=len(train_loader), desc=f'Epoch {epoch+1}/{epochs}')\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            training_loss += loss.item()\n",
        "            train_accuracy += calculate_accuracy(outputs, labels)\n",
        "            pbar.set_postfix({'Train Loss': training_loss / (pbar.n + 1), 'Train Acc': train_accuracy / (pbar.n + 1)})\n",
        "            pbar.update(1)\n",
        "\n",
        "        pbar.close()\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_accuracy = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                val_loss += criterion(outputs, labels).item()\n",
        "                val_accuracy += calculate_accuracy(outputs, labels)\n",
        "\n",
        "        train_accuracy /= len(train_loader)\n",
        "        training_loss /= len(train_loader)\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy /= len(val_loader)\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Train_Loss: {training_loss:.4f},  Train_Acc: {train_accuracy:.4f},  Val_Loss: {val_loss:.4f},  Val_Accuracy: {val_accuracy:.4f}')\n",
        "        wandb.log({\"epoch\": epoch+1, \"train_loss\": training_loss, \"val_loss\": val_loss, \"val_accuracy\": val_accuracy, \"train_accuracy\": train_accuracy})\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "UVhcym0i8Qps"
      },
      "execution_count": null,
      "outputs": [],
      "id": "UVhcym0i8Qps"
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes',  # Random search method\n",
        "    'metric': {'goal': 'maximize', 'name': 'val_accuracy'},  # Metric to optimize\n",
        "    'parameters': {\n",
        "        'epochs': {'values':[5, 10]},\n",
        "        'batch_size': {'values':[32, 64]},\n",
        "        'num_filters': {'values': [32, 64, 128]},\n",
        "        'activation': {'values': ['ReLU', 'GELU', 'SiLU', 'Mish']},\n",
        "        'filter_organization': {'values': ['same', 'double', 'halve']},\n",
        "        'data_augmentation': {'values': [True, False]},\n",
        "        'batch_norm': {'values': [True, False]},\n",
        "        'dropout': {'values': [0.2, 0.3]},\n",
        "        'strategy':{'values':[1, 2, 3]}\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "trusted": true,
        "id": "_c7l1Aiy8Qps"
      },
      "execution_count": null,
      "outputs": [],
      "id": "_c7l1Aiy8Qps"
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_additional_transforms(loader, additional_transforms, batch_size):\n",
        "    transformed_dataset = []\n",
        "    original_dataset = []\n",
        "    pbar = tqdm(total=len(loader))\n",
        "    for images, labels in loader:\n",
        "        images1 = additional_transforms(images)\n",
        "        for i in range(batch_size):\n",
        "            original_dataset.append((images[i], labels[i]))\n",
        "            transformed_dataset.append((images1[i], labels[i]))\n",
        "        pbar.set_postfix()\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "    return original_dataset, transformed_dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "RNgatYrn8Qps"
      },
      "execution_count": null,
      "outputs": [],
      "id": "RNgatYrn8Qps"
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_data(data_augmentation, train_loader, batch_size):\n",
        "    if data_augmentation:\n",
        "        additional_transforms = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
        "            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "        ])\n",
        "\n",
        "        # Apply additional transformations to the new DataLoader\n",
        "        original_dataset, transformed_dataset = apply_additional_transforms(train_loader, additional_transforms, batch_size)\n",
        "        combined_dataset = ConcatDataset([original_dataset, transformed_dataset])\n",
        "\n",
        "        # Create a new DataLoader using the combined dataset\n",
        "        combined_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)\n",
        "    else:\n",
        "        combined_loader = train_loader\n",
        "    return combined_loader"
      ],
      "metadata": {
        "id": "RoaHluPh8Qpt"
      },
      "execution_count": null,
      "outputs": [],
      "id": "RoaHluPh8Qpt"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_CNN(num_filters, activation, filter_organization, data_augmentation, batch_norm, dropout, batch_size, epochs):\n",
        "\n",
        "    # Load pre-trained model (ResNet50)\n",
        "    model = torchvision.models.resnet50(pretrained=True)\n",
        "    model.to(device)\n",
        "    if strategy == 1:\n",
        "    # Strategy 1: Freeze all layers except the last layer\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        num_ftrs = model.fc.in_features\n",
        "        model.fc = nn.Linear(num_ftrs, 101)  # 101 classes in iNaturalist\n",
        "    elif strategy == 2:\n",
        "    # Strategy 2: Freeze layers up to a certain depth\n",
        "    # Freeze layers up to layer 4\n",
        "        for name, param in model.named_parameters():\n",
        "            if 'layer4' not in name:  # Freeze layers up to layer 4\n",
        "                param.requires_grad = False\n",
        "    else:\n",
        "    # Strategy 3: Layer-wise fine-tuning\n",
        "        for param in model.layer4.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Create DataLoader instances for train and validation sets using the samplers\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n",
        "    val_loader = DataLoader(train_data, batch_size=batch_size, sampler=val_sampler)\n",
        "\n",
        "    combined_loader = augment_data(data_augmentation, train_loader, batch_size)\n",
        "\n",
        "    model = training_model(epochs, optimizer, criterion, model, combined_loader, val_loader)\n",
        "    return model"
      ],
      "metadata": {
        "id": "tNu8V9bI8Qpt"
      },
      "execution_count": null,
      "outputs": [],
      "id": "tNu8V9bI8Qpt"
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login(key = \"1d2c93cf7ddd48a63114848b66796301171827b6\")\n",
        "sweep_id = wandb.sweep(sweep_config, project='DL-Assignment-2')\n",
        "\n",
        "# Define your training function\n",
        "def train():\n",
        "\n",
        "    # Initialize Wandb run with custom run name\n",
        "    with wandb.init() as run:\n",
        "\n",
        "        # Use wandb.config to access hyperparameters in your training script\n",
        "        config = wandb.config\n",
        "        num_filters = config['num_filters']\n",
        "        activation = config['activation']\n",
        "        filter_organization = config['filter_organization']\n",
        "        data_augmentation = config['data_augmentation']\n",
        "        batch_norm = config['batch_norm']\n",
        "        batch_size = config['batch_size']\n",
        "        epochs = config['epochs']\n",
        "        dropout = config['dropout']\n",
        "        # Generate a custom run name based on hyperparameters\n",
        "        run_name = \"Part-B_\" + \"epochs_\" + str(epochs) + \"_nFilters_\" + str(num_filters) + \"_activation_\" + str(activation)+ \"_filterOrg_\" + str(filter_organization) + \"_batchSize_\" + str(batch_size)\n",
        "        wandb.run.name = run_name\n",
        "\n",
        "        model = train_CNN(num_filters, activation, filter_organization, data_augmentation, batch_norm, dropout, batch_size, epochs)\n",
        "\n",
        "        # Test the model\n",
        "        test_loader = DataLoader(test_data, batch_size=batch_size)\n",
        "        model.eval()\n",
        "\n",
        "        test_accuracy = 0.0\n",
        "        pbar = tqdm(total=len(test_loader))\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                images.to(\"cpu\")\n",
        "                labels.to(\"cpu\")\n",
        "                for i in range(len(images)):\n",
        "                    image = images[i]\n",
        "                    label = labels[i]\n",
        "                    output = outputs[i].argmax(dim = 0)\n",
        "                    if (label == output):\n",
        "                        test_accuracy += 1\n",
        "                pbar.set_postfix()\n",
        "                pbar.update(1)\n",
        "\n",
        "        pbar.close()\n",
        "        wandb.login(key = \"1d2c93cf7ddd48a63114848b66796301171827b6\")\n",
        "        with wandb.init( project='DL-Assignment-2') as run:\n",
        "            run_name = \"test_accuracy - Part B\"\n",
        "            wandb.run.name = run_name\n",
        "            test_accuracy /= len(test_data)\n",
        "            print(test_accuracy)\n",
        "            wandb.log({\"test_accuracy\": test_accuracy})\n",
        "\n",
        "        wandb.finish()\n",
        "\n",
        "# Run the sweep\n",
        "wandb.agent(sweep_id, function=train, count=20)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "tm548m_Y8Qpt"
      },
      "execution_count": null,
      "outputs": [],
      "id": "tm548m_Y8Qpt"
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "tKndI5SZ8Qpt"
      },
      "execution_count": null,
      "outputs": [],
      "id": "tKndI5SZ8Qpt"
    }
  ]
}